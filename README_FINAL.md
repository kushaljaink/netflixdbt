
# ğŸ¬ Netflix Analytics Engineering Project (dbt + Snowflake)

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end Analytics Engineering workflow** using **dbt** and **Snowflake**, built on top of the **MovieLens dataset** to simulate a real-world **Netflix-like analytics platform**.

The objective of this project is to showcase how raw data can be transformed into **analytics-ready fact and dimension tables** using modern data engineering best practices.

---

## ğŸ§  Business Context
Netflix-style platforms generate massive volumes of data such as:
- User ratings and engagement events
- Movie metadata and genres
- User-generated tags and feedback
- External movie references (IMDb, TMDB)

Analytics teams rely on clean, reliable, and well-modeled datasets to power dashboards, experimentation, and downstream machine learning use cases.

---

## ğŸ—ï¸ Architecture Overview

MovieLens CSV Files  
â†’ Amazon S3 (Raw Storage)  
â†’ Snowflake RAW Schema  
â†’ dbt Staging Models  
â†’ dbt Dimension & Fact Models  
â†’ Analytics Consumption + dbt Docs

---

## ğŸ›ï¸ Architecture Diagram

```mermaid
flowchart LR
    A[MovieLens CSV Files] --> B[Amazon S3]
    B --> C[Snowflake RAW Schema]

    C --> D[dbt Staging Models]
    D --> E[dbt Dimension Models]
    D --> F[dbt Fact Models]

    E --> G[Analytics / BI Consumption]
    F --> G

    E --> H[dbt Docs]
    F --> H
```

### Architecture Notes
- Raw CSV data is stored in **Amazon S3**
- Data is loaded into **Snowflake RAW schema**
- **dbt staging models** clean and standardize raw data
- **Dimension and fact models** support analytics use cases
- **dbt Docs** provides lineage and documentation

---

## ğŸ“‚ Dataset Used
**MovieLens 20M Dataset**

Files:
- `ratings.csv` â€“ User ratings with timestamps  
- `movies.csv` â€“ Movie titles and genres  
- `tags.csv` â€“ User-generated tags  
- `links.csv` â€“ External references (IMDb, TMDB)  
- `genome-tags.csv` â€“ Tag definitions  
- `genome-scores.csv` â€“ Tag relevance scores per movie  

---

## âš™ï¸ Tools & Technologies
- dbt Core
- Snowflake
- Amazon S3
- SQL
- YAML
- GitHub

---

## ğŸ§© Project Structure

```
netflixdbt/
â”‚
â”œâ”€â”€ analyses/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ dimensions/
â”‚   â””â”€â”€ facts/
â”‚
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ 01_project_structure.png
â”‚   â”œâ”€â”€ 02_dbt_run_success.png
â”‚   â”œâ”€â”€ 03_snowflake_raw_tables.png
â”‚   â”œâ”€â”€ 04_snowflake_dev_schema.png
â”‚   â”œâ”€â”€ 05_dbt_docs_lineage.png
â”‚   â””â”€â”€ 06_dbt_docs_model.png
â”‚
â”œâ”€â”€ seeds/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ packages.yml
â””â”€â”€ README.md
```

---

## ğŸ§± Data Modeling Layers

### 1ï¸âƒ£ Raw Layer
- CSV files loaded into Snowflake without modification
- Acts as a permanent source of truth

### 2ï¸âƒ£ Staging Layer (`src_*`)
- Column renaming
- Data type casting
- Basic data quality cleanup

### 3ï¸âƒ£ Dimension Tables
- `dim_movies`
- `dim_users`
- `dim_genome_tags`

### 4ï¸âƒ£ Fact Tables
- `fct_ratings`
- `fct_genome_scores`

---

## ğŸ§  Key dbt Models & SQL Logic

Below are selected SQL snippets from this project that demonstrate how dbt models were built
using staging layers, reusable references, and dimensional modeling best practices.

### ğŸ”¹ Staging Model â€“ Movies (`src_movies.sql`)

```sql
select
    movie_id,
    title,
    genres
from {{ source('netflix', 'movies') }}
```

---

### ğŸ”¹ Dimension Model â€“ Movies (`dim_movies.sql`)

```sql
{{ config(materialized='table') }}

select
    movie_id,
    title,
    genres
from {{ ref('src_movies') }}
```

---

### ğŸ”¹ Fact Model â€“ Ratings (`fct_ratings.sql`)

```sql
{{ config(materialized='incremental') }}

select
    user_id,
    movie_id,
    rating,
    rating_timestamp
from {{ ref('src_ratings') }}

{% if is_incremental() %}
where rating_timestamp >
    (select max(rating_timestamp) from {{ this }})
{% endif %}
```

---

### ğŸ”¹ Dimension Model â€“ Users (`dim_users.sql`)

```sql
with ratings_users as (
    select distinct user_id from {{ ref('src_ratings') }}
),
tag_users as (
    select distinct user_id from {{ ref('src_tags') }}
)

select distinct user_id
from ratings_users
union
select distinct user_id
from tag_users
```

---

## ğŸ–¥ï¸ dbt Commands Used (VS Code Terminal)

Below are the key dbt commands used during development, testing, and documentation of this project.

### ğŸ”¹ Initialize dbt Project
```bash
dbt init netflixdbt
```
Creates the dbt project structure and configuration files.

---

### ğŸ”¹ Install dbt Dependencies
```bash
dbt deps
```
Installs dbt packages defined in `packages.yml`.

---

### ğŸ”¹ Run All Models
```bash
dbt run
```
Executes all dbt models (staging, dimensions, facts) in dependency order.

---

### ğŸ”¹ Run a Specific Model
```bash
dbt run --select dim_movies
```
Runs an individual model for targeted development and debugging.

---

### ğŸ”¹ Run Models with Upstream Dependencies
```bash
dbt run --select +fct_ratings
```
Runs a model along with all its upstream dependencies.

---

### ğŸ”¹ Test Data Quality
```bash
dbt test
```
Executes schema tests such as `not_null` and `unique` defined in `schema.yml`.

---

### ğŸ”¹ Generate dbt Documentation
```bash
dbt docs generate
```
Builds metadata and lineage information for all models.

---

### ğŸ”¹ Serve dbt Docs Locally
```bash
dbt docs serve
```
Launches an interactive documentation website locally to explore models and lineage.

---


## ğŸ“¸ Screenshots

### ğŸ”¹ Project Folder Structure
![Project Structure](screenshots/01_project_structure.png)

### ğŸ”¹ dbt Run â€“ Successful Execution
![dbt Run](screenshots/02_dbt_run_success.png)

### ğŸ”¹ Snowflake RAW Tables
![Snowflake Raw](screenshots/03_snowflake_raw_tables.png)

### ğŸ”¹ Snowflake DEV Schema
![Snowflake Dev](screenshots/04_snowflake_dev_schema.png)

### ğŸ”¹ dbt Docs â€“ Lineage Graph
![dbt Lineage](screenshots/05_dbt_docs_lineage.png)

---

## ğŸ“˜ dbt Documentation

```bash
dbt docs generate
dbt docs serve
```

This launches an interactive documentation site showing:
- Model lineage
- Column descriptions
- Dependencies
- Source freshness

---

## ğŸš€ How to Run the Project

1. Upload MovieLens CSV files to Amazon S3  
2. Load raw data into Snowflake  
3. Initialize the dbt project  
4. Configure Snowflake profile  
5. Run transformations:
```bash
dbt run
```

---

## ğŸ¯ What This Project Demonstrates
- End-to-end analytics engineering workflow
- dbt best practices and modular modeling
- Snowflake-based data warehouse design
- Dimensional modeling (facts & dimensions)
- Documentation-driven analytics

---

## ğŸ“Œ Future Enhancements
- Incremental models for large fact tables
- dbt tests for data quality
- CI/CD using GitHub Actions
- BI tool integration (Power BI / Tableau)

---

## ğŸ‘¤ Author
**Kushal Jain**  
MS in Information Systems (Business Analytics)  
Analytics Engineering | Data Engineering | BI
